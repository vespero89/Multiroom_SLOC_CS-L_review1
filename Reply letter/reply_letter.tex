\documentclass[11pt, technote, letterpaper, oneside, onecolumn]{IEEEtran}
\usepackage{amssymb,amsmath}
\usepackage{multirow}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{tabularx}
\usepackage{array}
\usepackage{verbatim}
%\usepackage{color}
\usepackage{url}
\usepackage{algorithmicx}
\usepackage[ruled]{algorithm}
\usepackage{algpseudocode}
\usepackage{stmaryrd}
\usepackage{amsfonts}
\usepackage{textcomp}
\usepackage{tikz}
\usepackage{nth}
\usepackage{multirow}
\usepackage{mathtools}
\usepackage{xcolor}

\newcommand{\figref}[1]{Fig.~\ref{#1}}
\begin{document}


% Introduce a new counter for counting the nodes needed for circling
\newcounter{nodecount}
% Command for making a new node and naming it according to the nodecount counter
\newcommand\tabnode[1]{\addtocounter{nodecount}{1}\tikz \node (\arabic{nodecount}) {#1};}

\textbf{RESPONSE TO REVIEWERS}


\vspace{+2\baselineskip}

\noindent Dear Editor,

\vspace{+0.1\baselineskip}

first of all we would like to thank the reviewers for contributing with their insightful comments. We have revised the manuscript according to them, and detailed replies are listed below to identify the applied changes.  We really appreciate the review work which helped us to improve the scientific impact of the paper and its presentation quality. 
\vspace{+0.5\baselineskip}

\noindent Best regards,

\vspace{+0.1\baselineskip}

The Authors

\vspace{+3\baselineskip}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage

\section{Answers to reviewer 1}\label{sec:rev1}
\begin{enumerate}
\item \textit{Given that a map of the environment is available, it would be interesting to see the performance of a ray-tracing style algorithm. It should be possible to find the intersection of all the paths given by the peaks in each microphone pair GCC-PHAT output. If it is not feasible to carry out this experiment, I would suggest a dicussion of this kind of method and refer to literature.\\}
\textbf{Reply}: Thank you for your comment. It is authors' opinion that the CSP-SLOC algorithm described in Section 3.1 and considered in the comparative evaluation algorithm is a ray-tracing style  as suggested by the reviewer. CSP-SLOC (Tsiami et al. ``Experiments in acoustic source localization using sparse arrays in adverse indoors environments,'' in Proc. of EUSIPCO, 2014) belongs to the family of TDOA-based locators: it considers a room equipped with several microphone and estimates the speaker's position by calculating the intersection between the Direction of Arrivals (DOAs) curves obtained from each array. The DOAs are estimated by using the Crosspower Spectrum Phase Coherence Measure, and the ambiguity deriving from the presence of multiple intersection points is solved with a Least Squares solution that provides a Maximum Likelhood location estimate. The innovative contributions of the solution proposed by Tsiami et al. are pre-processing the input signals with a cepstral dereverberation algorithm, and the elimination of possible outliers. It is worth pointing out that Tsiami et al. evaluated their solution on the DIRHA dataset, the same used in our experiments. More in details, the Least Square estimate adopted by Tsiami et al. takes into account that in real environments the DOAs do not perfectly intersect at the same point. Please, let us know if we misinterpreted this comment as we will provide a more appropriate response. In order to clarify this aspect, the Section 1.1 (Related works) and the description of the CSP-SLOC algorithm (Section 3.1) has been modified accordingly.

%\textcolor{red}{TODO} - Fabio - looking for implementations of such algorithms - discuss in literature

\item \textit{Assuming that the microphones are calibrated, a simple way of determining speaker position is to used the signal strength. This would also be valuable to include in a discussion.\\}
\textbf{Reply}: %Thank you for your suggestion. 
Up to the authors' knowledge, approaches based on the signal strength (or signal energy) are usually adopted when wireless acoustic sensor networks (WASNs) are employed. The motivation is that in WASNs, sensors synchronization is not guaranteed, thus using TDOA and SRP-based methods is intrinsically not possible. On the contrary, energy-based approaches exploit an average energy measure and an energy decay model, thus they can be adopted when sensors are not synchronized. Their disadvantage, however, is their inferior accuracy with respect to TDOA and SRP-based methods, mainly due to the multiple propagation paths of the sound waves, and to channel fading. Approaches based on the signal energy have been described in Section 1.1 (Related Work). 

\begin{quote}
\textcolor{red}{
Energy-based approaches localize speakers by using the energy measures of the signals acquired from the acoustic sensors [23, 24]. More in details, these techniques are based on the average of the signals energy calculated over several windows of the signals themselves. Energy-based methods have been gaining significant popularity in wireless acoustic sensor networks scenarios, since they do not require the sensors to be precisely synchronized. Their disadvantage, however, is their inferior accuracy with respect to TDOA-based and SRP-based methods, mainly due to the multiple propagation paths of the sound waves, and to channel fading [23]. Additionally, source position is estimated by using an averaged measure, instead of a sample-by-sample or frame-by-frame information, thus making these techniques intrinsically less precise [23]. For a survey on energy-based localization methods, the interested reader can refer to [23, 24].}
\end{quote}

%\textcolor{red}{TODO}: Discussion about the use of energy as feature for the DNN (maybe for the M-VAD), the direct use of signal strength is quite inapplicable due to the not-regular disposition of the microphones

\item \textit{It would be nice to have the problem more clearly defined earlier in the paper, e.g., single- or multi-speaker localization, moving or stationary, are signals from all rooms used simultaneously, are there more than one microphone pair etc.\\}
\textbf{Reply}: % Thank you for your comment. 
 These aspects have been clarified by adding the following sentence at the end of Section 1 (Introduction):

\begin{quote}
\textcolor{red}{
In particular, this work addresses the task of localizing multiple moving speakers inside a home equipped with several microphones for each room. The algorithm is based on Deep Neural Networks (DNN) and it is able to simultaneously use the signals coming from all the rooms of the building. The proposed approach operates on Generalized Cross Correlation-PHAse Transform (GCC-PHAT) Patterns [14] features calculated from frames 30\,ms long and overlapped by 20\,ms. The speakers' positions are estimated every 10\,ms, and such a granular information can be directly exploited by algorithms that operate at the same time scale, such as beamformers [15]. The output can be processed further in order to estimate speakers' locations at a coarser time scale, for example for being used by reasoning algorithms and dialogue managers, that need a general contextual information [16]. This aspect has not been addressed in this paper, and it will be investigated in future works. The following section provides a review of the recent literature on the topic.
}
\end{quote}

Additionally, the first paragraph of Section 1.2 (Contribution) has been modified as follows:
	\begin{quote}
		\textcolor{red}{
		The main contribution of this work is the development of a completely data-driven approach for Speaker Localization (SLOC) in a multi-room environment, considering both the moving and the stationary conditions. 		The purpose of the data-driven strategy is to avoid a dedicated fine-tuning of parameters, which is typical and highly specific for the state of the art algorithms.
		In details, the proposed algorithm is composed of a feature extraction stage and an artificial neural network, composing together the DNN-SLOC. The feature extraction stage calculates GCC-PHAT Patterns [14] from pairs of microphone signals, and two different neural networks architectures are evaluated for estimating the speakers' position. The first network, denoted as MLP-SLOC, is composed of fully connected layers with rectified linear units (ReLU) [41]. The second network, denoted as CNN-SLOC, is composed of convolutional layers followed by fully connected layers with ReLUs. The motivation behind the use of convolutional layers resides in their ability to capture the intrinsic structure of GCC-PHAT Pattern matrices [14], which is lost by using fully connected layers only. Additionally, two different algorithm architectures are evaluated: in the first, localization in a room is performed by using only the signals acquired with the microphones present in that room. The second architecture uses the signals acquired with the microphones of all rooms. In the experiments, we investigated which combination of microphones yielded the best performance, as well as the importance of the temporal context.
		%Additionally to investigating different network architectures, the concurrent processing of one or more room audio data, the dependence on the microphone position and the importance of a temporal context are also investigated.
		}
	\end{quote}

\item \textit{What are the outputs of the algorithms? Coordinates? How to make sure, that only valid coordinates are chosen?\\}
\textbf{Reply}: Thank your comment. The outputs of the algorithms are the Cartesian coordinates of the speaker position inside the room. They are originally expressed in millimeters, but for training and testing they are normalized in the range $[0,1]$. During the localization phase, the actual predicted position in millimeters is calculated by reversing the normalization procedure. The origin of the Cartesian coordinate system is located in the upper left corner of a room. Regarding the validity of the coordinates, during our experiment the network predicted always values in the range $[0,1]$. However, in order to ensure that only valid predictions are considered, the final coordinate is calculated as $\tilde{x} =\max(0, \min(1, x))$, where $x$ can represent the abscissa $\chi$ or the ordinate $\psi$.  In order to clarify this aspect, Section 2 (Proposed Method) has been modified as follows:
%\begin{quote}
%	\text{Hence, the artificial neural network is trained on labelled data to estimate the coordinates $(\chi,\psi)$ i.e., the position of the speaker inside the target room.}
%\end{quote}
%into:
\begin{quote}
	\textcolor{red}{Hence, the artificial neural network is trained on labeled data to estimate the Cartesian coordinates $\left ( \chi,\psi \right )$, i.e., the position of the speaker inside the target room, with the origin of the coordinates system located in the upper left corner of a room. The target positions are originally expressed in millimeters and they are then scaled in the range $[0,1]$ for training and testing. In order to ensure that only valid predictions are produced, the final coordinate is calculated as $\tilde{x} =\max(0, \min(1, x))$, where $x$ can represent the abscissa $\chi$ or the ordinate $\psi$. Finally, the output is scaled back to obtain the coordinate in millimeters.}
\end{quote}
Additionally, the origin of the coordinates system has been added to Fig. 1.


\item \textit{Did the authors experiment with the frame ? 480ms seems quite high to me and seems to violate the assumption of the aqcuired signal to be almost stationary within a frame. In my experience 64-128 is more appropriate.\\}
\textbf{Reply}: Thank you for your remark. The frame size was erroneously reported as 480\,ms, however the actual value is 30\,ms and the hop size is 10\,ms. Considering that the sample rate is 16\,kHz, they are respectively equal to 480 samples and 160 samples. We have modified Section 2.1 (Features based on GCC-PHAT Patterns) as follows:

%\begin{quote}
%	\textbf{Hence, the GCC-PHAT Patterns are extracted as follows: for each considered microphone pair the CSPCM is computed with a frame and a hop respectively equal to 480\,ms and 160\,ms.}
%\end{quote}
%into:
\begin{quote}
	\textcolor{red}{Hence, the GCC-PHAT Patterns are extracted as follows: for each considered microphone pair the CSPCM is computed with a frame size of 30\,ms and a hop size of 10\,ms respectively equal to 480 samples and 160 samples at the sample rate of 16\,kHz.}
\end{quote}


\item \textit{It would also be interesting to discuss the applications of such systems and how it might impact the solution. For some applications it is not necessary to have a frame-level output.\\}
\textbf{Reply}: Thank you for your suggestion. The proposed approach, as well as the methods included in the comparative evaluation, estimate the position of a speaker for each time frame. In the case of the proposed approach, the frame size is 30\,ms and the frame period is 10\,ms, thus the localization output is given every 10\,ms. Regarding the CSP-SLOC and the SRP-SLOC, the frame sizes are respectively 30\,ms and 128\,ms, and the frame period is 10\,ms for both the algorithms. As pointed out by the reviewer, the need for such granularity in the localization output depends on the application scenario. %Generally, speaker localization algorithms are use in distant speech interaction systems, in particular where the system is constantly operating. 
Generally the information on the speaker position is important in distant speech interaction systems, in particular for beamforming algorithms, and dialogue managers and reasoning systems (M.\ W\"olfel, J.\ Mc-Donough, ``Distant speech recognition. John Wiley \& Sons, 2009, Brutti, Alessio, Mirco Ravanelli, and Maurizio Omologo. ``SASLODOM: Speech Activity detection and Speaker LOcalization in DOMestic environments.'' Proceedings of Evalita (2014)). Beamformers require the knowledge of the Direction of Arrival (DOA) of the source signal in order the filter undesired sources of noise, and frame level information is necessary since they process the input signal frame by frame. Dialogue managers and reasoning systems produce appropriate responses and sequences of actions based on the context in general, and on the speaker position in particular (e.g., in a speech-interfaced home automation system, if the user utters ``Switch-on the light'', the system should switch on the closest light).  For these algorithms a more coarse information is sufficient, since they operate on a larger time scale (e.g., after a person finished uttering a command for a home automation system). As aforementioned, the algorithms evaluated in this paper estimate the speaker position for each time frame given at its input, thus they are an adequate solution for being used in beamforming algorithms (see for example X. Xiao et al., ``Deep beamforming networks for multi-channel speech recognition,'' in Proc. of ICASSP, Shanghai, 2016, pp. 5745-5749, where the authors coupled a neural network direction of arrival estimator with a filter and sum beamformer). However, for being used with algorithms that operate on a larger time scale, their output should be properly processed to match the time scale of the subsequent processing element. 
%for example, by means of a smoothing as suggested by the reviewer.
The discussion of this aspect has been included in the last paragraph of Section 1 (Introduction):
\begin{quote}
\textcolor{red} {
In particular, this work addresses the task of localizing multiple moving speakers inside a home equipped with several microphones for each room. The algorithm is based on Deep Neural Networks (DNN) and it is able to simultaneously use the signals coming from all the rooms of the building. The proposed approach operates on Generalized Cross Correlation-PHAse Transform (GCC-PHAT) Patterns [14] features calculated from frames 30\,ms long and overlapped by 20\,ms, thus the speakers' positions are estimated every 10\,ms. Such a granular information can be directly exploited by algorithms that operate at the same time scale, such as beamformers [15]. The output can be processed further in order to estimate speakers' locations at a coarser time scale, for example for being used by reasoning algorithms and dialogue managers, that need a general contextual information [16]. This aspect has not been addressed in this paper, and it will be investigated in future works. The following section provides a review of the recent literature on the topic.
}
\end{quote}

 %Frame level localization information is particularly important for beamforming algorithms, since they require that position of the desired source is known. Speech recognition accuracies depend on localization accuracies.

%\textcolor{red}{TODO}: discussion - every algorithm has a frame level output and no post-processing procedure has been implemented, although it could be beneficial for the localization accuracy 
%This work was not directly addressed to a real-life application of such systems, but we want to demonstrate that a complete data-driven approach to the speaker localization with no post-processing procedure is able to obtain better performances in term of localization accuracy compared to the state of the art algorithms.

\item \textit{It would be nice to have a motivation for introducing convolutional networks. Why might it be expected to improve performance? I know that it can probably only be speculations, but as it is now, it appears a bit like trial-and-error.\\}
\textbf{Reply}: Thank you for your comment. The proposed localization algorithm is based on GCC-PHAT Patterns, originally proposed in Xiao et al., 2015 (reference 14). As shown in Xiao et al., 2015, the GCC-PHAT Patterns matrices exhibit a significant structure that can be exploited by a localization algorithm. Using a fully connected layer as first layer of the network as done by Xiao et al., 2015 requires flattening the feature matrix in a vector, thus loosing the structure information contained in GCC-PHAT Patterns. The motivation for using convolutional neural networks resides in their ability to directly process GCC-PHAT Pattern matrices and to capture their local correlations. In order to better motivate the choice of using convolutional layers, the first paragraph of Section 1.2 (Contribution) has been modified as follows:
\begin{quote}
\textcolor{red}{
The main contribution of this work is the development of a completely data-driven approach for Speaker Localization (SLOC) in a multi-room environment, considering both the moving and the stationary conditions. %}
%%%
The purpose of the data-driven strategy is to avoid a dedicated fine-tuning of parameters, which is typical and highly specific for the state of the art algorithms.
In details, the proposed algorithm is composed of a feature extraction stage and an artificial neural network, composing together the DNN-SLOC. %\textcolor{red}{
The feature extraction stage calculates GCC-PHAT Patterns [14] from pairs of microphone signals, and two different neural networks architectures are evaluated for estimating the speakers' position. The first network, denoted as MLP-SLOC, is composed of fully connected layers with rectified linear units (ReLU) [41]. The second network, denoted as CNN-SLOC, is composed of convolutional layers followed by fully connected layers with ReLUs. The motivation behind the use of convolutional layers resides in their ability to capture the intrinsic structure of GCC-PHAT Pattern matrices [14], which is lost by using fully connected layers only. Additionally, two different algorithm architectures are evaluated: in the first, localization in a room is performed by using only the signals acquired with the microphones present in that room. The second architecture uses the signals acquired with the microphones of all rooms. In the experiments, we investigated which combination of microphones yielded the best performance, as well as the importance of the temporal context.
%Additionally to investigating different network architectures, the concurrent processing of one or more room audio data, the dependence on the microphone position and the importance of a temporal context are also investigated.
}
\end{quote}

\item \textit{When comparing the two architectures, where one considers all signals from all (both) rooms vs only signals from the room with the source, how is the room with the source determined, i.e., how do you know which rooms contains the source/speaker?\\}\label{resp:vad}
\textbf{Reply}: Thank you for your comment. A preliminary stage of the proposed speaker localization algorithm is a multi-room voice activity detector (VAD) able to discriminate between silence and speech portions as conventional VADs as well as to identify the room where the speaker is present. In this paper, the focus is on estimating the speakers' position inside the room where the VAD identifies the presence of a source, thus we assumed the presence of an Oracle multi-room VAD able to perform the task without errors. As reported also in a previous work by the authors, Vesperini et al. ``A neural network based algorithm for speaker localization in a multi-room environment,'' in Proc. of MLSP, 2016, the development of a complete system able to perform both the tasks of the multi-room VAD and of the multi-room speaker localization presented in this paper will be addressed in future works.

In order to clarify this aspect and better describe the architecture of the proposed approach, Fig.~1 has been modified by including the multi-room VAD and the following sentence has been included in Section 2 (Proposed Method):
\begin{quote}
\textcolor{red}{
In a preliminary stage, a multi-room VAD [43, 60] extracts the speech portions of the signals and identifies the room where the speakers are located. Here, the multi-room VAD is supposed ideal: the development of a single algorithm able to perform both tasks simultaneously will be addressed in future works.}
\end{quote}

Additionally, in Section 6 (Conclusion and Outlook) the sentence:
\begin{quote}
The algorithm implicitly requires an Oracle VAD in order to process only human
speech.
\end{quote}

has been replaced with the sentence:
\begin{quote}
\textcolor{red}{The algorithm implicitly requires an Oracle multi-room VAD that identifies the time boundaries of the audio signals and the source room.}
 \end{quote}
 
\item \textit{Evaluation: It would be very interesting to have a plot of all the source/target locations on a map, to see their distribution.\\}
\textbf{Reply}: Thank you for your suggestion. The distributions of the ground truth positions and of the ones estimated by the most performing approach (CNN-SLOC) have been included and discussed in Sections 5.2.3 and 5.3.3, respectively for the Simulated scenario (Figure 9) and the Real scenario (Figure 13).

\item \textit{Evaluation: Does the baseline methods, CSP and SRP, use any smoothing or temporal context or are they simply evaluated frame by frame? Based on my experience a simple histogram technique computed over a few seconds can improve robustness quite a lot.\\} \label{resp:smooth}
\textbf{Reply}: Thank you for your suggestion. In the original implementation, no localization method adopted a technique for smoothing the estimated positions. However, SRP-SLOC operates on a wider temporal context with respect to CSP-SLOC and the proposed approaches without temporal context, since speaker localization is performed on frames composed of 2048 samples (about 4 frames of the DNN-based algorithm). As suggested by the reviewer, we investigated two smoothing methods as a final processing step of all the algorithms: median filter and moving average filter. For the sake of conciseness, we included only the results related to the median filter, since it exhibited the lowest RMSE with all the algorithms. Regarding the DNN-based approaches, the smoothing technique has been evaluated only on the most performing ones, i.e., CNN-SLOC 1Rx1N with temporal context in the Simulated scenario and CNN-SLOC 2Rx1N with temporal context in the Real scenario. For all the algorithms, we calculated the results obtained by using four windows sizes ($\{3,7,11,5\}$) and we reported the best results.

The smoothing technique has been described in Section 2.5 (Smoothing). The related results have been included in Table 7 and Table 12, and discussed in Sections 5.2.3 and 5.3.3.

\item \textit{Evaluation: How was the 500mm threshold determined?\\}
\textbf{Reply}: Thank you for your comment. This threshold and in general the evaluation metrics have been chosen according to the guidelines of the Speech Activity detection and Speaker LOcalization in DOMestic environments task of the EVALITA 2014 challenge. We added these details and the respective reference in Section 5.1 - Experimental Setup.
\begin{quote}
	\textcolor{red}{The performance metrics and the related hyperparameters have been chosen according to the guidelines of the Speech Activity detection and Speaker LOcalization in DOMestic environments (SASLODOM 2014) task of the EVALITA 2014 challenge [53].}
\end{quote}

\item \textit{Evaluation: What was the number for $N_{TOT}$?\\}
\textbf{Reply}: Thank you for your comment. The values of $N_{TOT}$ have been reported in Table 3.

\item \textit{Evaluation: Line 358: The explanation about the boxplot could be expanded to explain things better\\}
\textbf{Reply}: Thank you for your comment. In the boxplots included in the original version of the paper,  the minimum and maximum values of the colored box represented respectively the mean value minus half the standard deviation and the mean value plus half the standard deviation, and the horizontal line represented the mean value. The lower and upper whiskers represented respectively the lowest RMSE and highest RMSE. In this revised version, we changed the boxplots in order to be more compliant with their common definition: now the minimum and maximum values of the colored box represent respectively the first and the third quartile, while the horizontal line represents the median value. The lower and upper whiskers represent respectively the lowest RMSE within the 1.5 interquartile range of the lower quartile, and the highest RMSE within the 1.5 interquartile range of the upper quartile.

Additionally, in Section 5.2.1 (Evaluation without the temporal context) the results related to the boxplots have been discussed by including the following paragraph:
\begin{quote}
\textcolor{red}{It is interesting to analyze the behavior of the CNN-SLOC and MLP-SLOC algorithms in the two architecture variants for different number of microphone pairs, i.e., at the end of the GCC-PHAT Patterns selection stage. Figure 6 shows a box plot describing the overall results at the end of this stage for the Living Room: the minimum and maximum values of the colored box represent respectively the first and the third quartile, while the horizontal line represents the median value. The lower and upper whiskers represent respectively the lowest RMSE within the 1.5 interquartile range of the lower quartile, and the highest RMSE within the 1.5 interquartile range of the upper quartile. The marks represent values outside the whiskers range. % The boxplots show a certain robustness of the 2Rx1N architecture to the increased number of microphone pairs. Indeed, in the 1Rx1N architecture, the total number of microphone pairs
% dispersion of results for both the CNN and MLP architectures. 
Observing the results related to the MLP-SLOC algorithm, additionally to a reduction of the RMSE, the 2Rx1N architecture reduces also the dependence on the microphones pairs inside the room. Indeed, the standard deviation of the 1Rx1N is equal to 26.08\,mm, while for the 2Rx1N it reduces to 18.52\,mm. In the CNN-SLOC algorithm, the performance improvement is more consistent, since using the 2Rx1N reduces the RMSE by 15.56\,mm. The statistical behavior, however, is different from the one observed for the MLP-SLOC algorithm: in this case, the standard deviation increases by 9.51\,mm with respect to the 1Rx1N architecture.}
\end{quote}

\item \textit{Evaluation: Fig. 8b (Kitchen), What happens at C=17? This seems very strange.\\}
\textbf{Reply}: Thank you for your remark. Indeed, for $C=17$ and $C=19$ we erroneously reported the RMSE values obtained in early experiments. The figure has been modified by reporting the correct values.

\item \textit{Line 395: From what figure/numbers can I deduce this? It seems unclear, and what is temporal resolution in this case? Equal to C?\\}
\textbf{Reply}: Thank you for your comment. The sentence has been modified as follows:
\begin{quote}
\textcolor{red}{Observing the results related to the MLP-SLOC (Figure 12a), it can be noticed that increasing the context size $C$ up to 15 (stride equal to 3), 17 (stride equal to 5), or 19 (stride 1 and 4) reduces the RMSE. Regarding the behavior for different values of the stride, a part from the results related to stride equal to 1, the performance difference is negligible, since it amounts at most to 1.93\,mm. Similar considerations apply to the behavior of CNN-SLOC for different sizes of the temporal context and of the stride (Figure 12b).}
\end{quote}

 %The temporal resolution of the input values is equal to $C\cdot s$. Cf. Section 5.2.2 

\item \textit{The best performance is found when context corresponding to 8.5\,s is considered, which in my opinion is quite a lot for some applications but for others it is acceptable. This should be addressed.\\}
\textbf{Reply}:  Thank you for noticing this. The context size was erroneously reported as equal to 8.5\,s, however for the simulated dataset (Section 5.2.2) the actual value is 0.83\,s both for the kitchen and for the living room. For the real dataset, the actual value is 0.75\,s for the kitchen and 0.51\,s for the living room. The context size is calculated as $\left [s\cdot H \cdot(C-1)+L \right ]/f_s$, where $C$ is the number of frames, $s$ is the stride, $H=160$ is the hop size in samples,  $L=480$ is the analysis frame length in samples, and $f_s=16$\,kHz is the sample rate. The error has been corrected in Section 5.2.2. and Section 5.3.2. Additionally, the following sentence has been added to Section 2.2 (Temporal Context)

\begin{quote}
\textcolor{red}{The total length of the chunk in seconds is calculated as $\left [s\cdot H \cdot(C-1)+L \right ]/f_s$, where $H$ is the hop size in samples, and $L$ is the analysis frame length in samples.}
\end{quote}

%This was another typo. Considering that the temporal context is equal to $C\cdot s$, for the best case it results equal to 85 frames, which corresponds to 0.85\,s - cf. Section 5.2.2. The same is for the evaluations on the Real dataset, cf. Section 5.3.2.

\item \textit{Evaluation: In the case of real environment (Tab. 6) SRP performs equally well with the proposed methods when not considering temporal context. Could a simple extension of the SRP with maybe a histogram approach be considered? I would expect it to yield an improvement.\\}
\textbf{Reply}: Thank you for your suggestion. As suggested by the reviewer, we integrated a smoothing technique in the comparative algorithms and in the proposed approach in order to exploit the information contained in multiple adjacent frames. Please refer to comment nr.\ \ref{resp:smooth} for a detailed response.

% \textcolor{red}{TODO} - Check SRP code and its use of temporal context. Maybe some input frames can be aggregated??
%In the SRP phat algorithm we use a frame size equal to  (approximately 4 frame w.r.t. the DNN inputs), which corresponds to 128 ms at 16kHz sample rate.

\item \textit{line 188: How are the frames containing speech found?\\}
\textbf{Reply}: Please refer to response to comment number \ref{resp:vad} for a detailed discussion.

\item \textit{Bullet 2 on line 321: I do not completely understand what is going on here, and it is quite difficult to understand from the text. Please elaborate and motivate it.\\}
\textbf{Reply}: The kitchen and the living room considered in the experiments contain multiple microphones grouped in several linear arrays and one circular array. Referring to equations 6 and 7, the final feature matrix is obtained by stacking the GCC-PHAT Patterns calculated by using the microphone pairs of the individual arrays of a room. The objective of the GCC-PHAT Patterns selection is to choose the composition of the final matrix $\mathbf{X}[n]$ (equation 7), i.e., which pairs of microphones provide the best localization performance. The evaluation procedure is performed separately for each room and it starts by considering the pairs of the circular array on the ceiling. The related matrix is then augmented by gradually including the GCC-PHAT Patterns of the pairs of microphones of the remaining arrays. If the inclusion of the GCC-PHAT Pattern results in a performance improvement, it is retained, otherwise it is discarded. %Considering the kitchen (Fig. 5), three linear arrays are present: A=[K1R, K1L], [K3L, K3C, K3R], and [K2L, K2R]. The microphones pairs are [K1R, K1L] from array A, [K3L, K3C], [K3L, K3R], and [K3C, K3R] from array B, and [K2L, K2R] from array C. The related GCC-Patterns are gradually included in the initial feature matrix composed of the GCC-Patterns calculated from the circular array.

This aspect has been clarified by rewriting the description of the GCC-PHAT Patterns selection procedure as follows:
\begin{quote}
 \textcolor{red}{The objective of this stage is to find the composition of the feature matrix $\mathbf{X}[n]$ defined in Eq. 7 that provides the best localization performance. The procedure is performed separately for each room and it starts by considering the circular arrays on the ceilings: each array is composed of 6 microphones, thus, excluding the central one, the number of microphone pairs is 10 and the size of the initial feature matrix is $10 \times 50$. This matrix is then augmented by gradually including the GCC-PHAT Patterns $\mathbf{\tilde{x}}_{ab}[n]$ (see Eq. 5) related to the pairs of microphones of the remaining arrays of the room. If the inclusion of the GCC-PHAT Pattern results in a performance improvement, it is retained, otherwise it is discarded.}
\end{quote}

\item \textit{Conclusion: To me, the conclusion is way too optimistic and biased. Two main points to challenge are:}
	\begin{enumerate}
		\item  \textit{It is true, that no tuning of parameters are needed once the network is ALREADY trained, but based on the experiments in this paper it is obvious that some tuning has been done to find the best performance. Furthermore, what are all these parameters in state-of-the art methods which need tuning? I would like to have a concrete example.}
		\item  \textit{The authors implicitly claim that this performance of the architectures can be generalised to all rooms when trained, however this is not demonstrated properly. An interesting experiment would be to choose test the same architecture on a set of two new rooms. This would at least give a hint to how it generalises.}
	\end{enumerate}
 \textbf{Reply}: Thank you for your comment. Regarding the first remark, the purpose of the authors was to emphasize that the proposed approach is less dependent on the characteristics of the environment with respect to the comparative algorithms. Indeed, the proposed approach, as well as the comparative algorithms, require tuning of their hyperparameters in order to obtain the lowest RMSE. The CSP-SLOC algorithm depends on the values of the exponential window scaling factor $\alpha$ and the averaging weighting coefficient $\mu$ for cepstral dereverberation. The SRP-SLOC algorithm requires  tuning the Stochastic Region Contraction parameters $J_0$ and $N_0$. The proposed approach requires choosing the hyperparameters of the network (the number of layers, the number of units per layer, the number and the size of the kernels in convolutional layers), the size of the context and the stride.  However, CSP-SLOC and SRP-SLOC require respectively the positions of the microphones and the room geometry, while the proposed approach implicitly learns this information from data. The choice of the microphone pairs is a common problem of all the algorithms. %, however in the proposed approach the performance obtained by using all the microphone pairs is similar, 
 Indeed, in order to obtain the lowest RMSE, we studied the performance of the proposed algorithm for different network topologies, and for different values of the context size and of the stride. The network topology depends mostly on the dataset available for training, in particular on its size, rather than on the characteristics of the environment. Regarding the other aspects, it is worth highlighting that in both scenarios (simulated and real), the highest RMSE of the CNN-SLOC with 2Rx1N architecture without using the temporal context and microphone selection is lower than the SRP-SLOC RMSE in both datasets. In the simulated dataset, the highest RMSE of the CNN-SLOC is 695\,mm (Fig. 6), while the RMSE of the SRP-SLOC is 981\,mm (Table 5). In the real dataset, the highest RMSE of the CNN-SLOC is 662\,mm (Fig. 10), while the RMSE of the SRP-SLOC is 793\,mm (Table 8). This shows that the microphone selection stage and the context evaluation stage are important for obtaining the best overall performance, but they are not fundamental for obtaining a significant performance improvement with respect to the comparative methods.
 
  %Additionally, during the experiments performed for finding the lowest RMSEs of the comparative algorithm, we registered a significant sensibility with respect to their hyperparameters, in particular of the SRP-SLOC $J_0$ and $N_0$, wit
%  It is authors' opinion that the hyperparameters related to the network topology are mostly dependent on the size of the dataset used for training.
 
%  As reported in the experiments (Fig....), the dependence on the microphone combination is modest, in particular for the 2Rx1N architecture. Regarding the context size, a fine tuning is surely important for achieving the best performance. It is worth pointing out, however, the specific value may depend on the environment (indeed, the best context size is different for the kitchen and the living room), however when used, it always provides lower RMSEs with respect to both CSP-SLOC and the SRP-SLOC.


Regarding the second remark, the authors agree with the reviewer on the importance of evaluating a model on data acquired in a building that differs from the one used in the training phase. However, up to the authors' knowledge, datasets similar to DIRHA, acquired in a real environment with rooms equipped with several microphones and various noise conditions, is not publicly available, thus it is not possible to properly evaluate the generalization capabilities of the proposed approach. Due to the importance of this aspect, it is authors' opinion that it deserves a specific study, since it would require creating a new dataset. This would also give us the chance to evaluate transfer learning strategies to adapt the neural network model to a new environment with little or no data. 

These remarks have been addressed by rewriting the last paragraphs of  Section 6 (Conclusion and Outlook).

Additionally, in Section 1.1 (Related works) the following sentence:
\begin{quote}
The aforementioned algorithms require a specific and accurate tuning of their system parameters, which leads to a lack of generality for the system itself. Alternatively, data-driven approaches are able to extract key features from the input signal independently from the setting of application. In sight of this, neural networks can tackle these problems as they provide an \textit{expert system} able to work with a minimal human involvement.
\end{quote}
has been replaced with:
\begin{quote}
\textcolor{red}{The aforementioned algorithms are based on physical models of the signal propagation, and generally require specific information on the application environment. Differently, data-driven approaches are able to exploit the information contained in data, and can potentially achieve superior discriminative capabilities, provided that enough data is available for training.}
\end{quote}

%\begin{quote}
%\textcolor{red}{Furthermore, due to a lack of an suitable dataset, the capabilities of the trained models to generalize on unseen environments will be evaluated, together with the evaluation of transfer learning methods \cite{Pan2010}.}
%\end{quote}
 
%Regarding the second remark, the main objective of the paper is to propose a multi-room speaker localization algorithm, which learns to estimate the speaker position directly from the data. As pointed out in Xiao et al, a disadvantage of state-of-the art approaches is their low performance in real environments, often due to inaccurate models. The advantage of learning-based approaches, such as the one proposed, is that they learn the model directly from data without any a-priori model, in this case a part from GCC-PHAT Patterns. Indeed, as remarked by the reviewer, the proposed speaker localization algorithm should be able to generalise to unseen buildings. However, the proposed approach has been trained and evaluated on the DIRHA dataset, that contains acoustic data related to a single building. Up to the authors' knowledge, a similar dataset for evaluating the generalisation capabilities of the algorithm is not publicly available. It is authors' opinion that this aspect deserves a specific work, since it would require creating a new dataset in a real environment. Additionally, one-shot and zero-shot learning strategies can be adopted in order to adapt the neural network model to a new environment with little or no data.
 
% 
% \begin{enumerate}
% 	\item \textcolor{red}{TODO} - discussion: parameters of CSP (room geometry: distance between microphone array and the opposite wall), cepstral dereverb: frame size, overlap, $\alpha, \mu$) and SRP (room geometry, microphone positions, FFT frame size, Stochastic Region Contraction params = $J_0, N_0$) - Si potrebbe rimarcare il fatto che con l'algoritmo ANN based la posizione dei microfoni è ininfluente e non sono state usate tecniche di dereverb - speech enhancement.
% 	
% 	 \textcolor{red}{\textbf{Reply}: ndPAO:As reported in the conclusion section, a fine tuning of the SRP-SLOC parameters has been performed. The performance of the state-of-the-art approach depends of the reverberation and noise level, thus its parameters can not be generalized.} 
% 	 
%% 	\item \textcolor{red}{TODO} - discussion: We don't have a sufficient amount of data from DIRHA dataset to test the generalization performance of the proposed algorithm. 
% \end{enumerate}

\item \textit{It seems like the authors put very little cost to the process of training such a system. In the current system, I believe, 64 minutes are used for training, at least for the simulated scenario. How is this expected to be handled in a commercial product, if it has to be trained specifically for that particular environment?\\}
\textbf{Reply}: Thank you for your comment. Considering only the segments of the signals containing voice, the size of the training set in the simulated scenario is about 32 minutes, while in the real scenario  is about 3 minutes. Despite not being a very large amount of data, the authors agree with the reviewer that in a commercial product the acquisition and labelling of new data can be difficult to handle, thus it should be minimized or totally avoided. However, as reported also in the previous reply, due to the lack of a suitable dataset, it was not possible to evaluate the proposed approach on data acquired in a building the differs from the one used in the trained set. This aspect will be addressed in future works, where a suitable dataset will be created, and the generalization capabilities of the algorithm evaluated. Additionally,  transfer learning strategies for adapting the trained models on new environments with a minimum amount of new data will be considered.

%\textcolor{red}{TODO} - discussion: some procedures, like transfer learning or pre-training of the networks can be implemented in a real world application, but necessarily the network have to be fine-tuned on the data taken from the target operation environment.

\item \textit{Minor comments: Line 454: ".. by means of close in time frames". I understand, but it is not proper English.\\}
\textbf{Reply}: Thank you for comment. The sentence in Section 6 (Conclusion and Outlook) has been modified as follows:
\begin{quote}
	\textcolor{red}{In details, the capability of the network to exploit the audio coming from all the rooms has been evaluated, and different numbers of adjacent frames have been concatenated to evaluate the influence of the temporal context.}
\end{quote}
\end{enumerate}

\newpage
\section{Answers to reviewer 2}\label{sec:rev2}
\begin{enumerate}
\item \textit{In page 7, line 134. "the coordinates $(\chi,\phi)$, i.e.," It is unclear what $\chi$ and $\phi$ represent. Please define the two symbols clearly and give their units. It seems very important due to that Eq. (15) defines the RMSE, and the unit of RMSE is millimeter (mm). $\phi$ is often denoted as angle.\\}
\textbf{Reply}: Thank your comment. A similar remark was made by Reviewer nr.\ 1, and the related reply is reported below.

 The outputs of the algorithms are the Cartesian coordinates of the speaker position inside the room. They are originally expressed in millimeters, but for training and testing they are normalized in the range $[0,1]$. During the localization phase, the actual predicted position in millimeters is calculated by reversing the normalization procedure. The origin of the Cartesian coordinate system is located in the upper left corner of a room. Regarding the validity of the coordinates, during our experiment the network predicted always values in the range $[0,1]$. However, in order to ensure that only valid predictions are considered, the final coordinate is calculated as $\tilde{x} =\max(0, \min(1, x))$, where $x$ can represent the abscissa $\chi$ or the ordinate $\psi$.  In order to clarify this aspect, Section 2 (Proposed Method) has been modified as follows:
%\begin{quote}
%	\textbf{Hence, the artificial neural network is trained on labelled data to estimate the coordinates $(\chi,\psi)$ i.e., the position of the speaker inside the target room.}
%\end{quote}
%into:
\begin{quote}
	\textcolor{red}{Hence, the artificial neural network is trained on labeled data to estimate the Cartesian coordinates $\left ( \chi,\psi \right )$, i.e., the position of the speaker inside the target room, with the origin of the coordinates system located in the upper left corner of a room. The target positions are originally expressed in millimeters and they are then scaled in the range $[0,1]$ for training and testing. In order to ensure that only valid predictions are produced, the final coordinate is calculated as $\tilde{x} =\max(0, \min(1, x))$, where $x$ can represent the abscissa $\chi$ or the ordinate $\psi$. Finally, the output is scaled back to obtain the coordinate in millimeters.}
\end{quote}
Additionally, the origin of the coordinates system has been added to Fig. 1.

\item  \textit{In page 9, "for each considered microphone pair the CSPCM is computed with a frame and a hop respectively equal to 480 ms and 160 ms." I doubt about the frame and the hop here, it is not common to compute the CSPCM using a half-second long data segment. There are several reasons for this. First, the speech is only quasi-stationary. Second, it is unnecessary to use so long data segment to obtain the GCC-PHAT pattern since the maximum delay is only 24 samples as pointed out in Eq. (3). Third, the tracking capability will reduce dramatically when using so long data segment.\\}
\textbf{Reply}: Thank you for your comment. A similar remark was made by Reviewer nr.\ 1, and the related reply is reported below.

The frame size was erroneously reported as 480\,ms, however the actual value is 30\,ms and the hop size is 10\,ms. Considering that the sample rate is 16\,kHz, they are respectively equal to 480 samples and 160 samples. We have modified Section 2 (Features based on GCC-PHAT Patterns) as follows:

%\begin{quote}
%	\textbf{Hence, the GCC-PHAT Patterns are extracted as follows: for each considered microphone pair the CSPCM is computed with a frame and a hop respectively equal to 480\,ms and 160\,ms.}
%\end{quote}
%into:
\begin{quote}
	\textcolor{red}{Hence, the GCC-PHAT Patterns are extracted as follows: for each considered microphone pair the CSPCM is computed with a frame size of 30\,ms and a hop size of 10\,ms respectively equal to 480 samples and 160 samples at the sample rate of 16\,kHz.}
\end{quote}


\item  \textit{It is unclear why 500mm is chosen as a threshold. Why not other values, like 1000mm?\\}
\textbf{Reply}: Thank you for your comment.  A similar remark was made by Reviewer nr.\ 1, and the related reply is reported below.

This threshold and in general the evaluation metrics have been chosen according to the guidelines of the Speech Activity detection and Speaker LOcalization in DOMestic environments task of the EVALITA 2014 challenge. We added these details and the respective reference in Section 5.1 - Experimental Setup.
\begin{quote}
	\textcolor{red}{The performance metrics and the related hyperparameters have been chosen according to the guidelines of the Speech Activity detection and Speaker LOcalization in DOMestic environments (SASLODOM 2014) task of the EVALITA 2014 challenge [56].}
\end{quote}

\item  \textit{GCC-PHAT can also be applied to speaker localization directly, why not compare the proposed algorithm with the GCC-PHAT-based localization. \\}
\textbf{Reply}: Thank you for your comment. It is authors' opinion that the CSP-SLOC algorithm described in Section 3.1 and considered in the comparative evaluation algorithm is a GCC-PHAT-based localization as suggested by the reviewer. The CSP-SLOC algorithm proposed in ``Tsiami et al. Experiments in acoustic source localization using sparse arrays in adverse indoors environments, in Proc. of EUSIPCO, 2014'' is based on the calculation of the GCC with PHAT normalization as reported in Equation 1, and the speaker's position is estimated by finding the point that minimizes the sum of the squared distances from the estimated DOA lines. The innovations introduced by Tsiami and colleagues are the cepstral dereverberation algorithm and the outlier elimination procedure, however the localization algirithm is still based on GCC-PHAT.

In order to clarify this aspect, the Section 1.1 (Related works) and the description of the CSP-SLOC algorithm (Section 3.1) has been modified accordingly.

\item  \textit{For the reverberant and noisy environments, it is common to suppress the noise and the reverberant components beforehand to improve the localization accuracy. Moreover, as we know, if we properly choose some time-frequency bins that have high CDR or SNR values, the localization accuracy can also be improved dramatically.\\}
\textbf{Reply}: Thank you for your suggestion. The authors agree with the reviewer on the potential performance improvement that can be obtained by pre-processing the acquired signals with noise suppression and dereverberation algorithms. However, the general idea of the presented experiments is to compare the proposed approach with reference speaker localization algorithms whose results can be easily reproduced by the scientific community. Indeed, the CSP-SLOC algorithm proposed in  ``Tsiami, A., et al (2014). Experiments in acoustic source localization using sparse arrays in adverse indoors environments. In Proc. of European Signal Processing Conference (pp.\ 2390-2394)'' and implemented in this work includes a cepstral dereverberation algorithm. Regarding the SRP-SLOC, we implemented the algorithm described in ``Do, H., et al. (2007). A real-time SRP-PHAT source location implementation using stochastic region contraction (SRC) on a large-aperture microphone array. In Proc. of ICASSP (Vol. 1, pp. 121-124)'' without adding additional processing. As suggested by the reviewer, pre-processing the microphone signals with noise suppression and dereverberation algorithms would almost certainly improve the localization performance. However it is authors' opinion that this would fall outside the objective of this paper, that is demonstrating that the proposed approach outperforms reference localization algorithms without additional processing.  %Moreover, the purpose of the experiments is to show the performance of the different algorithm implemented as described in their original work and without additional processing.

% we have implemented a cepstral dereverberation procedure, in order to improve the localization accuracy according to the author's specifications. In this work we highlight that the proposed method is able to already significantly overcome the reference algorithms with pretty standard features as input and no speech enhancement process. Certainly, the suppression of noise and reverberant components can improve the localization accuracy, but it falls outside the aim of this paper.

\item  \textit{Please compare the computational complexity of these algorithms.\\}
\textbf{Reply}: Thank you for your suggestion. The computational complexity of the proposed approach and of the comparative algorithms has been reported in the related sections. Additionally, we added Section 5.4 (Computational complexity) to Section 5 (Experiments) where the actual complexities are reported and discussed.

\end{enumerate}

\end{document}
