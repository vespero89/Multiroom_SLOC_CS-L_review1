\documentclass[11pt, technote, letterpaper, oneside, onecolumn]{IEEEtran}
\usepackage{amssymb,amsmath}
\usepackage{multirow}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{tabularx}
\usepackage{array}
\usepackage{verbatim}
\usepackage{color}
\usepackage{url}
\usepackage{algorithmicx}
\usepackage[ruled]{algorithm}
\usepackage{algpseudocode}
\usepackage{stmaryrd}
\usepackage{amsfonts}
\usepackage{textcomp}
\usepackage{tikz}
\usepackage{nth}
\usepackage{multirow}
\usepackage{mathtools}

\begin{document}


% Introduce a new counter for counting the nodes needed for circling
\newcounter{nodecount}
% Command for making a new node and naming it according to the nodecount counter
\newcommand\tabnode[1]{\addtocounter{nodecount}{1}\tikz \node (\arabic{nodecount}) {#1};}

\textbf{RESPONSE TO REVIEWERS}


\vspace{+2\baselineskip}

\noindent Dear Editor,

\vspace{+0.1\baselineskip}

first of all we would like to thank the reviewers for contributing with their insightful comments. We have revised the manuscript according to them, and detailed replies are listed below to identify the applied changes.  We really appreciate the review work which helped us to improve the scientific impact of the paper and its presentation quality. 
\vspace{+0.5\baselineskip}

\noindent Best regards,

\vspace{+0.1\baselineskip}

The Authors

\vspace{+3\baselineskip}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage

\section{Answers to reviewer 1}\label{sec:rev1}
\begin{itemize}
\item Given that a map of the environment is available, it would be interesting to see the performance of a ray-tracing style algorithm. It should be possible to find the intersection of all the paths given by the peaks in each microphone pair GCC-PHAT output. If it is not feasible to carry out this experiment, I would suggest a dicussion of this kind of method and refer to literature.\\
\textbf{Reply}:

\item Assuming that the microphones are calibrated, a simple way of determining speaker position is to used the signal strength. This would also be valuable to include in a discussion.\\
\textbf{Reply}:

\item It would be nice to have the problem more clearly defined earlier in the paper, e.g., single- or multi-speaker localization, moving or stationary, are signals from all rooms used simultaneously, are there more than one microphone pair etc.\\
\textbf{Reply}: The main contribution of this work is the development of a completely data-driven approach for a single Speaker Localization (SLOC) in a multi-room environment, considering both the moving and the stationary condition. We evaluated also the effect of combining source from multiple microphone pairs on the localization accuracy. Cf. Section 1.2 - Contribution 

\item What are the outputs of the algorithms? Coordinates? How to make sure, that only valid coordinates are chosen?\\
\textbf{Reply}: The output of the algorithm are the Cartesian coordinates $\left \langle \chi,\psi \right \rangle \left [mm\right ]$ of the speaker inside the target room. The target positions are scaled between 0 and 1, thus the linear output of the ANN produces always positive values in this range which are simply unscaled to the $\left [mm\right ]$ values at the end of the processing. Cf. Section 2 - Proposed Method

\item Did the authors experiment with the frame ? 480ms seems quite high to me and seems to violate the assumption of the aqcuired signal to be almost stationary within a frame. In my experience 64-128 is more appropriate.\\
\textbf{Reply}: It was a typo. We used a frame size of 30\,ms and a hop size of 10\,ms respectively equal to 480 samples and 160 samples at the sample rate of 16\,kHz. Cf. Section 2 - Features based on GCC-PHAT Patterns

\item It would also be interesting to discuss the applications of such systems and how it might impact the solution. For some applications it is not necessary to have a frame-level output.\\
\textbf{Reply}:

\item It would be nice to have a motivation for introducing convolutional networks. Why might it be expected to improve performance? I know that it can probably only be speculations, but as it is now, it appears a bit like trial-and-error.\\
\textbf{Reply}:

\item When comparing the two architectures, where one considers all signals from all (both) rooms vs only signals from the room with the source, how is the room with the source determined, i.e., how do you know which rooms contains the source/speaker?\\
\textbf{Reply}: The experiments have been performed by assuming the presence of an Oracle multi-room Voice Activity Detector (VAD), which selects only the speech portions of the audio signals and the room where is located the speaker. Cf. Section 5.1 - Experimental Setup

\item Evaluation: It would be very interesting to have a plot of all the source/target locations on a map, to see their distribution.\\
\textbf{Reply}:

\item Evaluation: Does the baseline methods, CSP and SRP, use any smoothing or temporal context or are they simply evaluated frame by frame? Based on my experience a simple histogram technique computed over a few seconds can improve robustness quite a lot.\\
\textbf{Reply}: The baseline methods have been evaluated frame by frame, as they were presented in the relative reference papers. \textbf{ndFAB: Dovremmo valutare di inserire questa tecnica e fare nuovi esperimenti?}

\item Evaluation: How was the 500mm threshold determined?\\
\textbf{Reply}: This threshold was chosen according to the guidelines of the Speech Activity detection and Speaker LOcalization in DOMestic environments (SASLODOM 2014 http://dirha.fbk.eu/SASLODOM2014) task of the EVALITA 2014 challenge. Cf. Section 5 - Experiments

\item Evaluation: What was the number for N\_TOT?\\
\textbf{Reply}: Cf. Table XX in Section 5 - Experiments

\item Evaluation: Line 358: The explanation about the boxplot could be expanded to explain things better\\
\textbf{Reply}: Indeed, by using the audio from both rooms reduces the dependence on the microphones location inside the room as it is proved by a reduced area between the mean plus variance and mean less variance values. In particular, in the case of the MLP there is also an absolute reduction of the error by comparing the minimum and maximum values of the RMSE. Cf. Section 5.2.1 - Evaluation without the temporal context

\item Evaluation: Fig. 8b (Kitchen), What happens at C=17? This seems very strange.\\
\textbf{Reply}: \textbf{ndFAB: It is strange also for us!}

\item Line 395: From what figure/numbers can I deduce this? It seems unclear, and what is temporal resolution in this case? Equal to C?\\
\textbf{Reply}: The temporal resolution of the input values is equal to $C\cdot s$. Cf. Section 5.2.2 

\item The best performance is found when context corresponding to 8.5s is considered, which in my opinion is quite a lot for some applications but for others it is acceptable. This should be addressed.\\
\textbf{Reply}:

\item Evaluation: In the case of real environment (Tab. 6) SRP performs equally well with the proposed methods when not considering temporal context. Could a simple extension of the SRP with maybe a histogram approach be considered? I would expect it to yield an improvement.\\
\textbf{Reply}: \textbf{ndFAB: Come sopra. Dovremmo valutare di inserire questa tecnica e fare nuovi esperimenti?}

\item line 188: How are the frames containing speech found?\\
\textbf{Reply}: The frames containing speech are indicated to the SLOC algorithm by a multi-room VAD stage which is assumed to operate before the proposed system.

\item Bullet 2 on line 321: I do not completely understand what is going on here, and it is quite difficult to understand from the text. Please elaborate and motivate it.\\
\textbf{Reply}: \textbf{ndFAB: It is a kind of features selection stage. Cf. Section 5.1}

\item Conclusion: To me, the conclusion is way too optimistic and biased. Two main points to challenge are:\\
 1: It is true, that no tuning of parameters are needed once the network is ALREADY trained, but based on the experiments in this paper it is obvious that some tuning has been done to find the best performance. Furthermore, what are all these parameters in state-of-the art methods which need tuning? I would like to have a concrete example.\\
 2: The authors implicitly claim that this performance of the architectures can be generalised to all rooms when trained, however this is not demonstrated properly. An interesting experiment would be to choose test the same architecture on a set of two new rooms. This would at least give a hint to how it generalises.\\
 \textbf{Reply}:

\item It seems like the authors put very little cost to the process of training such a system. In the current system, I believe, 64 minutes are used for training, at least for the simulated scenario. How is this expected to be handled in a commercial product, if it has to be trained specifically for that particular environment?\\
\textbf{Reply}:

\item Minor comments: Line 454: ".. by means of close in time frames". I understand, but it is not proper English.\\
\textbf{Reply}: "while different amounts of adjacent time frames have been concatenated to test the temporal context effect." Cf. Section 6 - Conclusion and Outlook
\end{itemize}

\newpage
\section{Answers to reviewer 2}\label{sec:rev2}
\begin{itemize}
\item In page 7, line 134. "the coordinates $(\chi,\phi)$, i.e.," It is unclear what $\chi$ and $\phi$ represent. Please define the two symbols clearly and give their units. It seems very important due to that Eq. (15) defines the RMSE, and the unit of RMSE is millimeter (mm). $\phi$ is often denoted as angle.\\
\textbf{Reply}:  The output of the algorithm are the Cartesian coordinates $\left \langle \chi,\psi \right \rangle \left [mm\right ]$ of the speaker inside the target room.
\item  In page 9, "for each considered microphone pair the CSPCM is computed with a frame and a hop respectively equal to 480 ms and 160 ms." I doubt about the frame and the hop here, it is not common to compute the CSPCM using a half-second long data segment. There are several reasons for this. First, the speech is only quasi-stationary. Second, it is unnecessary to use so long data segment to obtain the GCC-PHAT pattern since the maximum delay is only 24 samples as pointed out in Eq. (3). Third, the tracking capability will reduce dramatically when using so long data segment.\\
\textbf{Reply}:  It was a typo. We used a frame size of 30\,ms and a hop size of 10\,ms respectively equal to 480 samples and 160 samples at the sample rate of 16\,kHz. Cf. Section 2 - Features based on GCC-PHAT Patterns
\item  It is unclear why 500mm is chosen as a threshold. Why not other values, like 1000mm?\\
\textbf{Reply}: This threshold was chosen according to the guidelines of the Speech Activity detection and Speaker LOcalization in DOMestic environments (SASLODOM 2014 http://dirha.fbk.eu/SASLODOM2014) task of the EVALITA 2014 challenge. Cf. Section 5 - Experiments
\item  GCC-PHAT can also be applied to speaker localization directly, why not compare the proposed algorithm with the GCC-PHAT-based localization. \\
\textbf{Reply}:
\item  For the reverberant and noisy environments, it is common to suppress the noise and the reverberant components beforehand to improve the localization accuracy. Moreover, as we know, if we properly choose some time-frequency bins that have high CDR or SNR values, the localization accuracy can also be improved dramatically.\\
\textbf{Reply}:

\item  Please compare the computational complexity of these algorithms.\\
\textbf{Reply}:

\end{itemize}

\end{document}
