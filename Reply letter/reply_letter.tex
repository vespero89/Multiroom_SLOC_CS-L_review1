\documentclass[11pt, technote, letterpaper, oneside, onecolumn]{IEEEtran}
\usepackage{amssymb,amsmath}
\usepackage{multirow}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{tabularx}
\usepackage{array}
\usepackage{verbatim}
%\usepackage{color}
\usepackage{url}
\usepackage{algorithmicx}
\usepackage[ruled]{algorithm}
\usepackage{algpseudocode}
\usepackage{stmaryrd}
\usepackage{amsfonts}
\usepackage{textcomp}
\usepackage{tikz}
\usepackage{nth}
\usepackage{multirow}
\usepackage{mathtools}
\usepackage{xcolor}
\begin{document}


% Introduce a new counter for counting the nodes needed for circling
\newcounter{nodecount}
% Command for making a new node and naming it according to the nodecount counter
\newcommand\tabnode[1]{\addtocounter{nodecount}{1}\tikz \node (\arabic{nodecount}) {#1};}

\textbf{RESPONSE TO REVIEWERS}


\vspace{+2\baselineskip}

\noindent Dear Editor,

\vspace{+0.1\baselineskip}

first of all we would like to thank the reviewers for contributing with their insightful comments. We have revised the manuscript according to them, and detailed replies are listed below to identify the applied changes.  We really appreciate the review work which helped us to improve the scientific impact of the paper and its presentation quality. 
\vspace{+0.5\baselineskip}

\noindent Best regards,

\vspace{+0.1\baselineskip}

The Authors

\vspace{+3\baselineskip}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage

\section{Answers to reviewer 1}\label{sec:rev1}
\begin{enumerate}
\item \textit{Given that a map of the environment is available, it would be interesting to see the performance of a ray-tracing style algorithm. It should be possible to find the intersection of all the paths given by the peaks in each microphone pair GCC-PHAT output. If it is not feasible to carry out this experiment, I would suggest a dicussion of this kind of method and refer to literature.\\}
\textbf{Reply}: \textcolor{red}{TODO} - Fabio - looking for implementations of such algorithms - discuss in literature

\item \textit{Assuming that the microphones are calibrated, a simple way of determining speaker position is to used the signal strength. This would also be valuable to include in a discussion.\\}
\textbf{Reply}: Thank you for your suggestion. Approaches based on the signal energy have been described in Section 1.1 (Related Work). 

\begin{quote}
\textcolor{red}{
Energy-based approaches localize speakers by using the energy measures of the signals acquired from the acoustic sensors \cite{Cobos2017,Meng2017}. More in details, these techniques are based on the average of the signals energy calculated over several windows of the signals themselves. Energy-based methods have been gaining significant popularity in wireless acoustic sensor networks scenarios, since they do not require a precise synchronization of the sensors. Their disadvantage, however, is their inferior accuracy with respect to TDOA-based methods, mainly due to the multiple propagation paths of the sound waves, and to channel fading \cite{Cobos2017}. Additionally, source position is estimated by using an averaged measure, instead of a sample-by-sample or frame-by-frame information, thus making these techniques intrinsically less precise \cite{Cobos2017}. For a survey on energy-based localization methods, the interested reader can refer to \cite{Cobos2017,Meng2017}.}
\end{quote}

\textcolor{red}{TODO}: Discussion about the use of energy as feature for the DNN (maybe for the M-VAD), the direct use of signal strength is quite inapplicable due to the not-regular disposition of the microphones

\item \textit{It would be nice to have the problem more clearly defined earlier in the paper, e.g., single- or multi-speaker localization, moving or stationary, are signals from all rooms used simultaneously, are there more than one microphone pair etc.\\}
\textbf{Reply}: Thank you for your comment. These aspect have been clarified by adding the following sentence at the end of Section 1 (Introduction):

\begin{quote}
\textcolor{red}{
In particular, this work addresses the task of localizing multiple moving speakers inside a home equipped with several microphones for each room. The algorithm is based on Deep Neural Networks (DNN) and it is able to simultaneously use the signals coming from all the rooms of the building. The proposed approach operates on Generalized Cross Correlation-PHAse Transform (GCC-PHAT) Patterns [14] features calculated from frames 30\,ms long and overlapped by 20\,ms, thus the speakers' positions are estimated every 10\,ms. Such a granular information can be directly exploited by algorithms that operate at the same time scale, such as beamformers [15]. The output can be processed further in order to estimate speakers' locations at a coarser time scale, for example for being used by reasoning algorithms and dialogue managers, that need a general contextual information [16]. This aspect has not been addressed in this paper, and it will be investigated in future works. The following section provides a review of the recent literature on the topic.
}
\end{quote}

Additionally, the first paragraph of Section 1.2 (Contribution) has been modified as follows:
	\begin{quote}
		\textcolor{red}{
		The main contribution of this work is the development of a completely data-driven approach for Speaker Localization (SLOC) in a multi-room environment, considering both the moving and the stationary conditions. 		The purpose of the data-driven strategy is to avoid a dedicated fine-tuning of parameters, which is typical and highly specific for the state of the art algorithms.
		In details, the proposed algorithm is composed of a feature extraction stage and an artificial neural network, composing together the DNN-SLOC. The feature extraction stage calculates GCC-PHAT Patterns \cite{xiao2015learning} from pairs of microphone signals, and two different neural networks architectures are evaluated for estimating the speakers' position. The first network, denoted as MLP-SLOC, is composed of fully connected layers with rectified linear units (ReLU) \cite{nair2010rectified}. The second network, denoted as CNN-SLOC, is composed of convolutional layers followed by fully connected layers with ReLUs. The motivation behind the use of convolutional layers resides in their ability to capture the intrinsic structure of GCC-PHAT Pattern matrices \cite{xiao2015learning}, which is lost by using fully connected layers only. Additionally, two different algorithm architectures are evaluated: in the first, localization in a room is performed by using only the signals acquired with the microphones present in that room. The second architecture uses the signals acquired with the microphones of all rooms. In the experiments, we investigated which combination of microphones yielded the best performance, as well as the importance of using a temporal context.
		%Additionally to investigating different network architectures, the concurrent processing of one or more room audio data, the dependence on the microphone position and the importance of a temporal context are also investigated.
		}
	\end{quote}

\item \textit{What are the outputs of the algorithms? Coordinates? How to make sure, that only valid coordinates are chosen?\\}
\textbf{Reply}: Thank your comment. The outputs of the algorithms are the Cartesian coordinates of the speaker position inside the room. They are originally expressed in millimeters, but for training and testing they are normalized in the range $[0,1]$. During the localization phase, the actual predicted position in millimeters is calculated by reversing the normalization procedure. The origin of the Cartesian coordinate system is located in the upper left corner of a room. Regarding the validity of the coordinates, during our experiment the network predicted always values in the range $[0,1]$. However, in order to ensure that only valid predictions are considered, the final coordinate is calculated as $\tilde{x} =\max(0, \min(1, x))$, where $x$ can represent the abscissa $\chi$ or the ordinate $\psi$.  In order to clarify this aspect, Section 2 (Proposed Method) has been modified as follows:
\begin{quote}
	\textbf{Hence, the artificial neural network is trained on labelled data to estimate the coordinates $(\chi,\psi)$ i.e., the position of the speaker inside the target room.}
\end{quote}
into:
\begin{quote}
	\textcolor{red}{Hence, the artificial neural network is trained on labeled data to estimate the Cartesian coordinates $\left ( \chi,\psi \right )$, i.e., the position of the speaker inside the target room, with the origin of the coordinates system located in the upper left corner of a room. The target positions are originally expressed in millimeters and they are then scaled in the range $[0,1]$ for training and testing. In order to ensure that only valid predictions are produced, the final coordinate is calculated as $\tilde{x} =\max(0, \min(1, x))$, where $x$ can represent the abscissa $\chi$ or the ordinate $\psi$. Finally, the output is unscaled to obtain the coordinate in millimeters.}
\end{quote}
Additionally, the origin of the coordinates system has been added to Fig. 1. \textcolor{red}{TODO!}


\item \textit{Did the authors experiment with the frame ? 480ms seems quite high to me and seems to violate the assumption of the aqcuired signal to be almost stationary within a frame. In my experience 64-128 is more appropriate.\\}
\textbf{Reply}: Thank you for your remark. The frame size was erroneously reported as 480\,ms, however the actual value is 30\,ms and the hop size is 10\,ms. Considering that the sample rate is 16\,kHz, they are respectively equal to 480 samples and 160 samples. We have modified Section 2 (Features based on GCC-PHAT Patterns) as follows:

\begin{quote}
	\textbf{Hence, the GCC-PHAT Patterns are extracted as follows: for each considered microphone pair the CSPCM is computed with a frame and a hop respectively equal to 480\,ms and 160\,ms.}
\end{quote}
into:
\begin{quote}
	\textcolor{red}{Hence, the GCC-PHAT Patterns are extracted as follows: for each considered microphone pair the CSPCM is computed with a frame size of 30\,ms and a hop size of 10\,ms respectively equal to 480 samples and 160 samples at the sample rate of 16\,kHz.}
\end{quote}


\item \textit{It would also be interesting to discuss the applications of such systems and how it might impact the solution. For some applications it is not necessary to have a frame-level output.\\}
\textbf{Reply}: Thank you for your suggestion. The proposed approach, as well as the methods included in the comparative evaluation, estimate the position of a speaker for each time frame. In the case of the proposed approach, the frame size is 30\,ms and the frame period is 10\,ms, thus the localization output is given every 10\,ms. Regarding the CSP-SLOC and the SRP-SLOC, the frame sizes are respectively 30\,ms and 128\,ms, and the frame period is 10\,ms for both the algorithms. As pointed out by the reviewer, the need for such granularity in the localization output depends on the application scenario. %Generally, speaker localization algorithms are use in distant speech interaction systems, in particular where the system is constantly operating. 
Generally the information on the speaker position is important in distant speech interaction systems, in particular for beamforming algorithms, and dialogue managers and reasoning systems (M.\ W\"olfel, J.\ Mc-Donough, ``Distant speech recognition. John Wiley \& Sons, 2009, Brutti, Alessio, Mirco Ravanelli, and Maurizio Omologo. "SASLODOM: Speech Activity detection and Speaker LOcalization in DOMestic environments." Proceedings of Evalita (2014)). Beamformers require the knowledge of the speaker position in order the filter undesired sources of noise, and frame level information is necessary since they operate at the same time scale of localization algorithms. Dialogue managers and reasoning systems produce appropriate responses and sequences of actions based on the context in general, and on the speaker position in particular (e.g., in a speech-interfaced home automation system, if the user utters ``Switch-on the light'', the system should switch on the closest light).  For these algorithms a more coarse information is sufficient, since they operate on a larger time scale (e.g., after a person finished uttering a command for a home automation system). As aforementioned, the algorithms evaluated in this paper estimate the speaker position for each time frame given at its input, thus they are an adequate solution for being used in beamforming algorithms (see for example X. Xiao et al., ``Deep beamforming networks for multi-channel speech recognition,'' in Proc. of ICASSP, Shanghai, 2016, pp. 5745-5749, where the authors coupled a neural network direction of arrival estimator with a filter and sum beamformer). However, for being used with algorithms that operate on a larger time scale, their output should be properly processed to match the time scale of the subsequent processing element. 
%for example, by means of a smoothing as suggested by the reviewer.
The discussion of this aspect has been included in the last paragraph of Section 1 (Introduction):
\begin{quote}
\textcolor{red} {
In particular, this work addresses the task of localizing multiple moving speakers inside a home equipped with several microphones for each room. The algorithm is based on Deep Neural Networks (DNN) and it is able to simultaneously use the signals coming from all the rooms of the building. The proposed approach operates on Generalized Cross Correlation-PHAse Transform (GCC-PHAT) Patterns [14] features calculated from frames 30\,ms long and overlapped by 20\,ms, thus the speakers' positions are estimated every 10\,ms. Such a granular information can be directly exploited by algorithms that operate at the same time scale, such as beamformers [15]. The output can be processed further in order to estimate speakers' locations at a coarser time scale, for example for being used by reasoning algorithms and dialogue managers, that need a general contextual information [16]. This aspect has not been addressed in this paper, and it will be investigated in future works. The following section provides a review of the recent literature on the topic.
}
\end{quote}

 %Frame level localization information is particularly important for beamforming algorithms, since they require that position of the desired source is known. Speech recognition accuracies depend on localization accuracies.

%\textcolor{red}{TODO}: discussion - every algorithm has a frame level output and no post-processing procedure has been implemented, although it could be beneficial for the localization accuracy 
%This work was not directly addressed to a real-life application of such systems, but we want to demonstrate that a complete data-driven approach to the speaker localization with no post-processing procedure is able to obtain better performances in term of localization accuracy compared to the state of the art algorithms.

\item \textit{It would be nice to have a motivation for introducing convolutional networks. Why might it be expected to improve performance? I know that it can probably only be speculations, but as it is now, it appears a bit like trial-and-error.\\}
\textbf{Reply}: Thank you for your comment. The proposed localization algorithm is based on GCC-PHAT Patterns, originally proposed in Xiao et al., 2015 (reference 35 (\textcolor{red}{TODO: controllare che il numero rimanga lo stesso!}). As shown in Xiao et al., 2015, the GCC-PHAT Patterns matrices exhibit a significant structure that can be exploited by a localization algorithm. Using a fully connected layer as first layer of the network as done by Xiao et al., 2015 requires flattening the feature matrix in a vector, thus loosing the structure information contained in GCC-PHAT Patterns. The motivation for using convolutional neural networks resides in their ability to directly process GCC-PHAT Pattern matrices and to capture their local correlations. In order to better motivate the choice of using convolutional layers, the first paragraph of Section 1.2 (Contribution) has been modified as follows:
\begin{quote}
\textcolor{red}{
The main contribution of this work is the development of a completely data-driven approach for Speaker Localization (SLOC) in a multi-room environment, considering both the moving and the stationary conditions. %}
%%%
The purpose of the data-driven strategy is to avoid a dedicated fine-tuning of parameters, which is typical and highly specific for the state of the art algorithms.
In details, the proposed algorithm is composed of a feature extraction stage and an artificial neural network, composing together the DNN-SLOC. %\textcolor{red}{
The feature extraction stage calculates GCC-PHAT Patterns \cite{xiao2015learning} from pairs of microphone signals, and two different neural networks architectures are evaluated for estimating the speakers' position. The first network, denoted as MLP-SLOC, is composed of fully connected layers with rectified linear units (ReLU) \cite{nair2010rectified}. The second network, denoted as CNN-SLOC, is composed of convolutional layers followed by fully connected layers with ReLUs. The motivation behind the use of convolutional layers resides in their ability to capture the intrinsic structure of GCC-PHAT Pattern matrices \cite{xiao2015learning}, which is lost by using fully connected layers only. Additionally, two different algorithm architectures are evaluated: in the first, localization in a room is performed by using only the signals acquired with the microphones present in that room. The second architecture uses the signals acquired with the microphones of all rooms. In the experiments, we investigated which combination of microphones yielded the best performance, as well as the importance of using a temporal context.
%Additionally to investigating different network architectures, the concurrent processing of one or more room audio data, the dependence on the microphone position and the importance of a temporal context are also investigated.
}
\end{quote}

\item \textit{When comparing the two architectures, where one considers all signals from all (both) rooms vs only signals from the room with the source, how is the room with the source determined, i.e., how do you know which rooms contains the source/speaker?\\}\label{resp:vad}
\textbf{Reply}: Thank you for your comment. A preliminary stage of the proposed speaker localization algorithm is a multi-room voice activity detector (VAD) able to discriminate between silence and speech portions as conventional VADs as well as to identify the room where the speaker is present. In this paper, the focus is on estimating the speakers' position inside the room where the VAD identifies the presence of a source, thus we assumed the presence of an Oracle multi-room VAD able to perform the task without errors. As reported also in a previous work by the authors, Vesperini et al. ``A neural network based algorithm for speaker localization in a multi-room environment,'' in Proc. of MLSP, 2016, the development of a complete system able to perform both the tasks of the multi-room VAD and of the multi-room speaker localization presented in this paper will be addressed in future works.

In order to clarify this aspect and better describe the architecture of the proposed approach, Fig.~1 (\textcolor{red}{TODO}) has been modified by including the multi-room VAD and the following sentence has been included in Section 2 (Proposed Method):
\begin{quote}
\textcolor{red}{
In a preliminary stage, a multi-room VAD [16,40] (\textcolor{red}{TODO: controllare che i numeri di ref rimangano gli stessi}) extracts the speech portions of the signals and identifies the room where the speakers are located. Here, the multi-room VAD is supposed ideal, and the development of an algorithm able to perform both tasks simultaneously will be addressed in future works.}
\end{quote}
 
\item \textit{Evaluation: It would be very interesting to have a plot of all the source/target locations on a map, to see their distribution.\\}
\textbf{Reply}: \textcolor{red}{TODO - In progress - PAOLO}

\item \textit{Evaluation: Does the baseline methods, CSP and SRP, use any smoothing or temporal context or are they simply evaluated frame by frame? Based on my experience a simple histogram technique computed over a few seconds can improve robustness quite a lot.\\}
\textbf{Reply}: All the methods have been evaluated frame by frame, thus a post processing technique can have beneficial effects also for the DNN. \textcolor{red}{TODO} - Evaluations for best cases with smoothing?

\item \textit{Evaluation: How was the 500mm threshold determined?\\}
\textbf{Reply}: Thank you for your comment. This threshold and in general the evaluation metrics have been chosen according to the guidelines of the Speech Activity detection and Speaker LOcalization in DOMestic environments task of the EVALITA 2014 challenge. We added these details and the respective reference in Section 5.1 - Experimental Setup.
\begin{quote}
	\textcolor{red}{The performance metrics and the related hyperparameters have been chosen according to the guidelines of the Speech Activity detection and Speaker LOcalization in DOMestic environments (SASLODOM 2014) task of the EVALITA 2014 challenge [53] (TODO: controllare che il numero rimanga lo stesso!).}
\end{quote}

\item \textit{Evaluation: What was the number for $N_{TOT}$?\\}
\textbf{Reply}: Cf. Table XX in Section 5 - Experiments \textcolor{red}{TODO}

\item \textit{Evaluation: Line 358: The explanation about the boxplot could be expanded to explain things better\\}
\textbf{Reply}: \textcolor{red}{TODO} - Explain better what the box-plots show.\\
Indeed, by using the audio from both rooms reduces the dependence on the microphones location inside the room as it is proved by a reduced area between the mean plus variance and mean less variance values. In particular, in the case of the MLP there is also an absolute reduction of the error by comparing the minimum and maximum values of the RMSE. Cf. Section 5.2.1 - Evaluation without the temporal context

\item \textit{Evaluation: Fig. 8b (Kitchen), What happens at C=17? This seems very strange.\\}
\textbf{Reply}: \textcolor{red}{TODO} - Check experiments

\item \textit{Line 395: From what figure/numbers can I deduce this? It seems unclear, and what is temporal resolution in this case? Equal to C?\\}
\textbf{Reply}: The temporal resolution of the input values is equal to $C\cdot s$. Cf. Section 5.2.2 

\item \textit{The best performance is found when context corresponding to 8.5\,s is considered, which in my opinion is quite a lot for some applications but for others it is acceptable. This should be addressed.\\}
\textbf{Reply}:  Thank you for noticing this. The context size was erroneously reported as equal to 8.5\,s, however for the simulated dataset (Section 5.2.2) the actual value is 0.83\,s both for the kitchen and for the living room. For the real dataset, the actual value is 0.75\,s for the kitchen and 0.51\,s for the living room. The context size is calculated as $\left [s\cdot P \cdot(C-1)+L \right ]/f_s$, where $C$ is the number of frames, $s$ is the stride, $P=160$ is the hop size in samples,  $L=480$ is the analysis frame length in samples, and $f_s=16$\,kHz is the sample rate. The error has been corrected in Section 5.2.2. and Section 5.3.2. Additionally, the following sentence has been added to Section 2.2 (Temporal Context)

\begin{quote}
\textcolor{red}{The total length of the chunk in seconds is calculated as $\left [s\cdot P \cdot(C-1)+L \right ]/f_s$, where $P$ is the hop size in samples, and $L$ is the analysis frame length in samples.}
\end{quote}

%This was another typo. Considering that the temporal context is equal to $C\cdot s$, for the best case it results equal to 85 frames, which corresponds to 0.85\,s - cf. Section 5.2.2. The same is for the evaluations on the Real dataset, cf. Section 5.3.2.

\item \textit{Evaluation: In the case of real environment (Tab. 6) SRP performs equally well with the proposed methods when not considering temporal context. Could a simple extension of the SRP with maybe a histogram approach be considered? I would expect it to yield an improvement.\\}
\textbf{Reply}: \textcolor{red}{TODO} - Check SRP code and its use of temporal context. Maybe some input frames can be aggregated??
In the SRP phat algorithm we use a frame size equal to  (approximately 4 frame w.r.t. the DNN inputs), which corresponds to 128 ms at 16kHz sample rate.

\item \textit{line 188: How are the frames containing speech found?\\}
\textbf{Reply}: Please refer to response to comment number \ref{resp:vad} for a detailed discussion.

\item \textit{Bullet 2 on line 321: I do not completely understand what is going on here, and it is quite difficult to understand from the text. Please elaborate and motivate it.\\}
\textbf{Reply}: The kitchen and the living room considered in the experiments contain multiple microphones grouped in several linear arrays and one circular array. Referring to equations 6 and 7, the final feature matrix is obtained by stacking the GCC-PHAT Patterns calculated by using the microphone pairs of the individual arrays of a room. The objective of the GCC-PHAT Patterns selection is to choose the composition of the final matrix $\mathbf{X}[n]$ (equation 7), i.e., which pairs of microphones provide the best localization performance. The evaluation procedure is performed separately for each room and it starts by considering the pairs of the circular array on the ceiling. The related matrix is then augmented by gradually including the GCC-PHAT Patterns of the pairs of microphones of the remaining arrays. If the inclusion of the GCC-PHAT Pattern results in a performance improvement, it is retained, otherwise it is discarded. %Considering the kitchen (Fig. 5), three linear arrays are present: A=[K1R, K1L], [K3L, K3C, K3R], and [K2L, K2R]. The microphones pairs are [K1R, K1L] from array A, [K3L, K3C], [K3L, K3R], and [K3C, K3R] from array B, and [K2L, K2R] from array C. The related GCC-Patterns are gradually included in the initial feature matrix composed of the GCC-Patterns calculated from the circular array.

This aspect has been clarified by rewriting the description of the GCC-PHAT Patterns selection procedure as follows:
\begin{quote}
 \textcolor{red}{The objective of this stage is to find the composition of the feature matrix $\mathbf{X}[n]$ defined in Eq. 7 that provides the best localization performance. The procedure is performed separately for each room and it starts by considering the circular arrays on the ceilings: each array is composed of 6 microphones, thus, excluding the central one, the number of microphone pairs is 10 and the size of the initial feature matrix is $10 \times 50$. This matrix is then augmented by gradually including the GCC-PHAT Patterns $\mathbf{\tilde{x}}_{ab}[n]$ (see Eq. 5) related to the pairs of microphones of the remaining arrays of the room. If the inclusion of the GCC-PHAT Pattern results in a performance improvement, it is retained, otherwise it is discarded.}
\end{quote}

\item \textit{Conclusion: To me, the conclusion is way too optimistic and biased. Two main points to challenge are:}
	\begin{enumerate}
		\item  \textit{It is true, that no tuning of parameters are needed once the network is ALREADY trained, but based on the experiments in this paper it is obvious that some tuning has been done to find the best performance. Furthermore, what are all these parameters in state-of-the art methods which need tuning? I would like to have a concrete example.}
		\item  \textit{The authors implicitly claim that this performance of the architectures can be generalised to all rooms when trained, however this is not demonstrated properly. An interesting experiment would be to choose test the same architecture on a set of two new rooms. This would at least give a hint to how it generalises.}
	\end{enumerate}
 \textbf{Reply}:
 \begin{enumerate}
 	\item \textcolor{red}{TODO} - discussion: parameters of CSP (Room geometry, cepstral dereverb) and SRP ($J_0, N_0, \dots$)
 	
 	 \textcolor{red}{\textbf{Reply}: \textbf{ndPAO:As reported in the conclusion section, a fine tuning of the SRP-SLOC parameters has been performed. The performance of the state-of-the-art approach depends of the reverberation and noise level, thus its parameters can not be generalized.} }
 	 
 	\item \textcolor{red}{TODO} - discussion: We don't have a sufficient amount of data from DIRHA dataset to test the generalization performance of the proposed algorithm. 
 \end{enumerate}

\item \textit{It seems like the authors put very little cost to the process of training such a system. In the current system, I believe, 64 minutes are used for training, at least for the simulated scenario. How is this expected to be handled in a commercial product, if it has to be trained specifically for that particular environment?\\}
\textbf{Reply}:
\textcolor{red}{TODO} - discussion: some procedures, like transfer learning or pre-training of the networks can be implemented in a real world application, but necessarily the network have to be fine-tuned on the data taken from the target operation environment.

\item \textit{Minor comments: Line 454: ".. by means of close in time frames". I understand, but it is not proper English.\\}
\textbf{Reply}: The sentence in Section 6 - Conclusion and Outlook has been modified as follows:
\begin{quote}
	\textit{In details, audio coming from one or two rooms has been jointly exploited (1Rx1N, 2Rx1N), while the temporal context has been tested by means of close in time frames.}
\end{quote}
into:
\begin{quote}
	\textcolor{red}{In details, audio coming from one or two rooms has been jointly exploited (1Rx1N, 2Rx1N), while different numbers of adjacent frames have been concatenated to evaluate the influence of the temporal context.}
\end{quote}
\end{enumerate}

\newpage
\section{Answers to reviewer 2}\label{sec:rev2}
\begin{enumerate}
\item \textit{In page 7, line 134. "the coordinates $(\chi,\phi)$, i.e.," It is unclear what $\chi$ and $\phi$ represent. Please define the two symbols clearly and give their units. It seems very important due to that Eq. (15) defines the RMSE, and the unit of RMSE is millimeter (mm). $\phi$ is often denoted as angle.\\}
\textbf{Reply}:  The output of the algorithms are the Cartesian coordinates of the speaker position inside the room. We have modified Section 2 - Proposed Method as follows:
\begin{quote}
	\textit{Hence, the artificial neural network is trained on labelled data to estimate the coordinates $()\chi,\psi)$ i.e., the position of the speaker inside the target room.}
\end{quote}
into:
\begin{quote}
	\textcolor{red}{\textbf{The output of the algorithm are the Cartesian coordinates $\left \langle \chi,\psi \right \rangle \left [mm\right ]$ of the speaker inside the target room. The target positions are scaled between 0 and 1, thus the linear output of the ANN produces always positive values in this range which are simply unscaled to the $\left [mm\right ]$ values at the end of the processing.}}
\end{quote}
\textcolor{red}{TODO} - Add origin of the axes?

\item  \textbf{In page 9, "for each considered microphone pair the CSPCM is computed with a frame and a hop respectively equal to 480 ms and 160 ms." I doubt about the frame and the hop here, it is not common to compute the CSPCM using a half-second long data segment. There are several reasons for this. First, the speech is only quasi-stationary. Second, it is unnecessary to use so long data segment to obtain the GCC-PHAT pattern since the maximum delay is only 24 samples as pointed out in Eq. (3). Third, the tracking capability will reduce dramatically when using so long data segment.\\}
\textbf{Reply}:  It was a typo. We used a frame size of 30\,ms and a hop size of 10\,ms respectively equal to 480 samples and 160 samples at the sample rate of 16\,kHz. We have modified Section 2 - Features based on GCC-PHAT Patterns as follows:

\begin{quote}
	\textit{Hence, the GCC-PHAT Patterns are extracted as follows: for each considered microphone pair the CSPCM is computed with a frame and a hop respectively equal to 480 ms and 160 ms.}
\end{quote}
into:
\begin{quote}
	\textcolor{red}{\textbf{Hence, the GCC-PHAT Patterns are extracted as follows: for each considered microphone pair the CSPCM is computed with a frame size of 30\,ms and a hop size of 10\,ms respectively equal to 480 samples and 160 samples at the sample rate of 16\,kHz.}}
\end{quote}


\item  \textbf{It is unclear why 500mm is chosen as a threshold. Why not other values, like 1000mm?\\}
\textbf{Reply}: This threshold and in general the evaluation metrics were chosen according to the guidelines of the Speech Activity detection and Speaker LOcalization in DOMestic environments task of the EVALITA 2014 challenge. We added these details ad the respective reference in Section 5.1 - Experimental Setup.
\begin{quote}
	\textit{Similarly to a classification task, $P_{cor}$ provides a measure of the localization accuracy, since the estimated positions with an RMSE less than 500\,mm represent the correct decisions. These two metrics are averaged over all the available network outputs.}
\end{quote}

\begin{quote}
	\textcolor{red}{\textbf{Similarly to a classification task, $P_{cor}$ provides a measure of the localization accuracy, since the estimated positions with an RMSE less than 500\,mm represent the correct decisions, according to the guidelines of the Speech Activity detection and Speaker LOcalization in DOMestic environments (SASLODOM 2014)\footnote{http://dirha.fbk.eu/SASLODOM2014} task of the EVALITA 2014 challenge \cite{basili2014proceedings}. These two metrics are averaged over all the available network outputs.}}
\end{quote}

\item  \textbf{GCC-PHAT can also be applied to speaker localization directly, why not compare the proposed algorithm with the GCC-PHAT-based localization. \\}
\textbf{Reply}:  It is exactly what we have done with the CSP-SLOC. \textcolor{red}{TODO} - explain the algorithm

\item  \textbf{For the reverberant and noisy environments, it is common to suppress the noise and the reverberant components beforehand to improve the localization accuracy. Moreover, as we know, if we properly choose some time-frequency bins that have high CDR or SNR values, the localization accuracy can also be improved dramatically.\\}
\textbf{Reply}: For the comparative CSP-SLOC approach we have implemented a cepstral dereverberation procedure, in order to improve the localization accuracy according to the author's specifications. In this work we highlight that the proposed method is able to already significantly overcome the reference algorithms with pretty standard features as input and no speech enhancement process. Certainly, the suppression of noise and reverberant components can improve the localization accuracy, but it falls outside the aim of this paper.

\item  \textbf{Please compare the computational complexity of these algorithms.\\}
\textbf{Reply}: \textcolor{red}{TODO} - Fabio

\end{enumerate}

\end{document}
